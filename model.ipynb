{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove non-numeric columns and target-related columns that shouldn't be features\n",
    "    exclude_cols = ['user_id', 'risk_category', 'primary_risk_drivers', \n",
    "                   'churn_score', 'estimated_days_to_churn', 'risk_velocity']\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(exclude_cols + ['is_likely_churn'], axis=1)\n",
    "    y = df['is_likely_churn']\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_and_prepare_data('data/events_with_churn_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X, y):\n",
    "    print(\"\\n1. Starting model training and evaluation...\")\n",
    "    print(\"2. Splitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(\"3. Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"   ✓ Data preprocessing completed\")\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameter grids\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n4. Training models with grid search...\")\n",
    "    for name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "        print(f\"\\n   Starting {name} classifier training...\")\n",
    "        start_time = time.time()\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='roc_auc')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Save best model\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = grid_search.predict(X_test_scaled)\n",
    "        y_pred_proba = grid_search.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name] = {\n",
    "            'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'feature_importance': None  # Will be filled later\n",
    "        }\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n   ✓ {name} Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"   Results for {name}:\")\n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - AUC Score: {results[name]['auc']:.4f}\")\n",
    "        print(f\"   - F1 Score: {results[name]['f1']:.4f}\")\n",
    "        print(\"\\n   Classification Report:\")\n",
    "        print(\"   \" + classification_report(y_test, y_pred).replace(\"\\n\", \"\\n   \"))\n",
    "    \n",
    "    return best_models, results, X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Starting model training and evaluation...\n",
      "2. Splitting data into train and test sets...\n",
      "3. Scaling features...\n",
      "   ✓ Data preprocessing completed\n",
      "\n",
      "4. Training models with grid search...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d170652ff44a73a751c2dc9de2642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Starting Random Forest classifier training...\n",
      "\n",
      "   ✓ Random Forest Training completed in 118.82 seconds\n",
      "   Results for Random Forest:\n",
      "   - Best parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "   - AUC Score: 0.9973\n",
      "   - F1 Score: 0.9779\n",
      "\n",
      "   Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "              0       0.98      0.96      0.97      1141\n",
      "              1       0.97      0.98      0.98      1530\n",
      "   \n",
      "       accuracy                           0.97      2671\n",
      "      macro avg       0.97      0.97      0.97      2671\n",
      "   weighted avg       0.97      0.97      0.97      2671\n",
      "   \n",
      "\n",
      "   Starting Gradient Boosting classifier training...\n",
      "\n",
      "   ✓ Gradient Boosting Training completed in 552.02 seconds\n",
      "   Results for Gradient Boosting:\n",
      "   - Best parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "   - AUC Score: 0.9979\n",
      "   - F1 Score: 0.9781\n",
      "\n",
      "   Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "              0       0.97      0.97      0.97      1141\n",
      "              1       0.98      0.98      0.98      1530\n",
      "   \n",
      "       accuracy                           0.97      2671\n",
      "      macro avg       0.97      0.97      0.97      2671\n",
      "   weighted avg       0.97      0.97      0.97      2671\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "best_models, results, X_train_scaled, X_test_scaled, y_train, y_test = train_and_evaluate_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(best_models, X, X_train_scaled, X_test_scaled, y_test):\n",
    "    # Get feature names\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Random Forest feature importance\n",
    "    rf_model = best_models['Random Forest']\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # SHAP values for Gradient Boosting\n",
    "    gb_model = best_models['Gradient Boosting']\n",
    "    explainer = shap.TreeExplainer(gb_model)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # For binary classification\n",
    "    \n",
    "    # Calculate mean absolute SHAP values for each feature\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return rf_importance, shap_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_importance, shap_importance = analyze_feature_importance(\n",
    "        best_models, X, X_train_scaled, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importance of Features (Random Forest):\n",
      "                 feature  importance\n",
      "39               int_rec    0.122319\n",
      "30         inactive_days    0.109559\n",
      "0                ses_rec    0.102569\n",
      "26            ses_wknd_r    0.054557\n",
      "32     peak_activity_day    0.053440\n",
      "28           time_to_int    0.031411\n",
      "24            ses_hr_avg    0.030997\n",
      "6                ses_n_r    0.028902\n",
      "33        off_hours_rate    0.027546\n",
      "7                  int_n    0.025637\n",
      "1            ses_rec_avg    0.025485\n",
      "40            int_rec_sd    0.025483\n",
      "31      peak_activity_hr    0.022467\n",
      "4               user_rec    0.021593\n",
      "11               int_n_r    0.017133\n",
      "14               rev_sum    0.017106\n",
      "27           ses_len_avg    0.016985\n",
      "13              tran_n_r    0.015278\n",
      "45           rev_per_int    0.015073\n",
      "8             view_count    0.014204\n",
      "20         int_cat_n_avg    0.013455\n",
      "25             ses_hr_sd    0.012913\n",
      "15      rev_per_purchase    0.012901\n",
      "12                tran_n    0.012866\n",
      "38            ses_gap_sd    0.012315\n",
      "22            ses_mo_avg    0.012163\n",
      "2             ses_rec_sd    0.011986\n",
      "10        purchase_count    0.011284\n",
      "16             rev_sum_r    0.010165\n",
      "48       cross_cat_ratio    0.009785\n",
      "34            ses_len_sd    0.009558\n",
      "47   pop_cat_consistency    0.008863\n",
      "18             int_cat_n    0.008465\n",
      "37       short_ses_ratio    0.008146\n",
      "23             ses_mo_sd    0.007942\n",
      "21         int_itm_n_avg    0.007876\n",
      "5                  ses_n    0.007864\n",
      "41      view_to_cart_avg    0.007342\n",
      "42  cart_to_purchase_avg    0.007326\n",
      "19             int_itm_n    0.006904\n",
      "3             ses_rec_cv    0.005052\n",
      "35            ses_len_cv    0.004666\n",
      "36        long_ses_ratio    0.004427\n",
      "43              int_skew    0.003473\n",
      "9             cart_count    0.002047\n",
      "29          time_to_tran    0.001547\n",
      "46  purchase_consistency    0.000728\n",
      "17         major_spend_r    0.000170\n",
      "44       high_spike_flag    0.000027\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImportance of Features (Random Forest):\")\n",
    "print(rf_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importance of Features (SHAP):\n",
      "                 feature  importance\n",
      "26            ses_wknd_r    1.150781\n",
      "0                ses_rec    0.994610\n",
      "39               int_rec    0.920906\n",
      "20         int_cat_n_avg    0.795104\n",
      "30         inactive_days    0.779311\n",
      "33        off_hours_rate    0.689454\n",
      "13              tran_n_r    0.657306\n",
      "25             ses_hr_sd    0.332848\n",
      "7                  int_n    0.329086\n",
      "15      rev_per_purchase    0.328420\n",
      "6                ses_n_r    0.289325\n",
      "11               int_n_r    0.287799\n",
      "48       cross_cat_ratio    0.266146\n",
      "1            ses_rec_avg    0.212648\n",
      "32     peak_activity_day    0.197808\n",
      "27           ses_len_avg    0.182911\n",
      "47   pop_cat_consistency    0.161831\n",
      "40            int_rec_sd    0.138429\n",
      "24            ses_hr_avg    0.132634\n",
      "43              int_skew    0.131183\n",
      "14               rev_sum    0.130167\n",
      "38            ses_gap_sd    0.127274\n",
      "2             ses_rec_sd    0.126941\n",
      "23             ses_mo_sd    0.122057\n",
      "28           time_to_int    0.120682\n",
      "19             int_itm_n    0.116922\n",
      "8             view_count    0.115630\n",
      "34            ses_len_sd    0.103001\n",
      "18             int_cat_n    0.092482\n",
      "3             ses_rec_cv    0.084420\n",
      "45           rev_per_int    0.081307\n",
      "10        purchase_count    0.069576\n",
      "5                  ses_n    0.066446\n",
      "35            ses_len_cv    0.063769\n",
      "12                tran_n    0.062161\n",
      "31      peak_activity_hr    0.055392\n",
      "21         int_itm_n_avg    0.053745\n",
      "4               user_rec    0.037991\n",
      "16             rev_sum_r    0.032232\n",
      "41      view_to_cart_avg    0.025666\n",
      "36        long_ses_ratio    0.023607\n",
      "22            ses_mo_avg    0.022112\n",
      "9             cart_count    0.021592\n",
      "37       short_ses_ratio    0.010546\n",
      "42  cart_to_purchase_avg    0.007025\n",
      "46  purchase_consistency    0.001848\n",
      "44       high_spike_flag    0.000488\n",
      "29          time_to_tran    0.000148\n",
      "17         major_spend_r    0.000046\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImportance of Features (SHAP):\")\n",
    "print(shap_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
